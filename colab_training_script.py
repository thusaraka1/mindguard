# -*- coding: utf-8 -*-
"""MindGuard_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/xxxx
"""

# ==========================================
# 1. SETUP & IMPORTS
# ==========================================
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

# Check for GPU
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
if len(tf.config.list_physical_devices('GPU')) > 0:
    print("ðŸš€ Training on GPU!")
else:
    print("âš ï¸ GPU not detected. Running on CPU.")

# ==========================================
# 2. LOAD DATA
# ==========================================
# UNCOMMENT the lines below if mounting from Google Drive
from google.colab import drive
import os
drive.mount('/content/drive')

# --- DEBUGGING: CHECK FILE LOCATION ---
print("Checking files in My Drive...")
if os.path.exists('/content/drive/My Drive'):
    files = os.listdir('/content/drive/My Drive')
    # Filter to show only excel/csv files to avoid clutter
    relevant_files = [f for f in files if 'mental' in f.lower()]
    print(f"Found related files in Drive: {relevant_files}")
else:
    print("Could not find 'My Drive'. Is it mounted correctly?")

# Adjust this path based on the output above!
file_path = '/content/drive/My Drive/mental_health_iot_dataset_2500.csv.xlsx'

# For direct upload to Colab session storage (Use this only if NOT using Drive):
# file_path = 'mental_health_iot_dataset_2500.csv.xlsx'

try:
    print(f"Attempting to read: {file_path}")
    df = pd.read_excel(file_path)
except:
    df = pd.read_csv(file_path)

print(f"Dataset Loaded. Shape: {df.shape}")

# ==========================================
# 3. PREPROCESSING
# ==========================================
print("\n--- Preprocessing ---")

# 3.1 Drop ID (irrelevant)
if 'Patient_ID' in df.columns:
    df = df.drop('Patient_ID', axis=1)

# 3.2 Encode Target Variable (Mental_State_Label)
le_target = LabelEncoder()
df['Mental_State_Label'] = le_target.fit_transform(df['Mental_State_Label'])
print(f"Target Classes: {le_target.classes_}") # ['Non-Normal', 'Normal'] likely maps to [0, 1]

# 3.3 Encode Categorical Features (Gender, Province)
# We use LabelEncoder for simplicity or One-Hot if cardinality is low. 
# Given Province has ~9 values, One-Hot is better for accuracy to avoid ordinal assumptions.
categorical_cols = ['Gender', 'Province']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)

# 3.4 Split Features and Target
X = df.drop('Mental_State_Label', axis=1).values
y = df['Mental_State_Label'].values

# 3.5 Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3.6 Feature Scaling (CRITICAL for Neural Networks)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Save the scaler for the real-time app!
joblib.dump(scaler, 'scaler.pkl')
print("Scaler saved as 'scaler.pkl'")

# ==========================================
# 4. MODEL ARCHITECTURE
# ==========================================
# Deep Neural Network optimized for T4 GPU and High Accuracy
def build_model(input_dim):
    model = keras.Sequential([
        # Input Layer
        layers.Input(shape=(input_dim,)),
        
        # Hidden Layer 1: High capacity
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3), # Prevent overfitting given small dataset (2500)
        
        # Hidden Layer 2
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # Hidden Layer 3
        layers.Dense(64, activation='relu'),
        layers.BatchNormalization(),
        
        # Output Layer (Binary Classification)
        layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

model = build_model(X_train.shape[1])
model.summary()

# ==========================================
# 5. TRAINING
# ==========================================
# Callbacks for best performance
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_accuracy', 
    patience=20, 
    restore_best_weights=True,
    verbose=1
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.5, 
    patience=10, 
    min_lr=1e-6,
    verbose=1
)

print("\n--- Starting Training ---")
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    batch_size=32, # Smaller batch size often generalizes better
    epochs=200,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# ==========================================
# 6. EVALUATION
# ==========================================
print("\n--- Evaluation ---")
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le_target.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le_target.classes_, 
            yticklabels=le_target.classes_)
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# ==========================================
# 7. SAVE MODEL
# ==========================================
import os
import datetime
import json

# Create a unique name based on Accuracy and Time
acc_str = f"{accuracy*100:.2f}"
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
base_name = f"mindguard_acc{acc_str}_{timestamp}"

# Define the Save Directory (Google Drive)
# Creates a folder "MindGuard_Models" in your Drive if it doesn't exist
save_dir = '/content/drive/My Drive/MindGuard_Models'
os.makedirs(save_dir, exist_ok=True)

print(f"\nðŸ’¾ Saving artifacts to: {save_dir}")
print(f"ðŸ“¦ Base Filename: {base_name}")

# 1. Save Model (.h5 and .keras)
model_path_h5 = os.path.join(save_dir, f"{base_name}.h5")
model_path_keras = os.path.join(save_dir, f"{base_name}.keras")
model.save(model_path_h5)
model.save(model_path_keras)
print(f" - Model saved: {model_path_h5}")

# 2. Save Scaler
scaler_path = os.path.join(save_dir, f"{base_name}_scaler.pkl")
joblib.dump(scaler, scaler_path)
print(f" - Scaler saved: {scaler_path}")

# 3. Save Metadata
metadata_path = os.path.join(save_dir, f"{base_name}_metadata.json")
# df already has One-Hot Encoding applied, so we take columns directly
feature_columns = list(df.drop('Mental_State_Label', axis=1).columns)

metadata = {
    "columns": feature_columns,
    "classes": list(le_target.classes_),
    "accuracy": accuracy,
    "timestamp": timestamp
}
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=4)
print(f" - Metadata saved: {metadata_path}")

print(f"\nâœ… SUCCESS! All files are safely in your Google Drive folder: 'MindGuard_Models'")
